{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the JSON data\n",
    "with open('training/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = [item['text'] for item in data]\n",
    "labels = [item['label'] for item in data]\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'training/model'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "max_len = 60\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_len)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=500, shuffle=False)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.08587588369846344\n",
      "Epoch: 0, Loss: 0.08258695900440216\n",
      "Epoch: 0, Loss: 0.10346461832523346\n",
      "Epoch: 0, Loss: 0.09246617555618286\n",
      "Epoch: 0, Loss: 0.1039210706949234\n",
      "Epoch: 0, Loss: 0.09753921627998352\n",
      "Epoch: 0, Loss: 0.0888405591249466\n",
      "Epoch: 0, Loss: 0.07822679728269577\n",
      "Epoch: 0, Loss: 0.0903550311923027\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation function\n",
    "import os\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += labels.size(0)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "    return accuracy.item()\n",
    "\n",
    "# Training loop with evaluation\n",
    "checkpoint_dir = 'training/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Function to save a checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, checkpoint_path)\n",
    "    print(f'Checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "# Training loop with checkpointing\n",
    "for epoch in range(50):  # Let's train for 50 epochs\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_accuracy = evaluate(model, val_dataloader)\n",
    "    print(f'Epoch: {epoch}, Validation Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # Save a checkpoint at the end of each epoch\n",
    "    save_checkpoint(model, optimizer, epoch, loss.item(), checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('training/trained_model/tokenizer_config.json',\n",
       " 'training/trained_model/special_tokens_map.json',\n",
       " 'training/trained_model/vocab.txt',\n",
       " 'training/trained_model/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('training/trained_model')\n",
    "tokenizer.save_pretrained('training/trained_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
